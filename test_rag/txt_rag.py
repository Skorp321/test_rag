import streamlit as st
import os
from openai import OpenAI
from langchain_openai import ChatOpenAI
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain_core.documents import Document
from langchain.chains import RetrievalQA
from dotenv import load_dotenv
import pandas as pd
import glob
import zipfile
import xml.etree.ElementTree as ET


load_dotenv()

class DocumentAgent:
    def __init__(self, openai_api_key):
        self.openai_api_key = openai_api_key
        self.llm = ChatOpenAI(
            openai_api_key=openai_api_key,
            openai_api_base="https://10f9698e-46b7-4a33-be37-f6495989f01f.modelrun.inference.cloud.ru/v1",
            model="qwen3:32b",
            temperature=0.1
        )
        self.embeddings = HuggingFaceEmbeddings(
            model_name="intfloat/multilingual-e5-large",
            model_kwargs={'device': 'cuda'}
        )
        self.vectorstore = None
        self.qa_chain = None
        
    def load_documents_from_folder(self, folder_path):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –ø–∞–ø–∫–∏"""
        try:
            all_documents = []
            
            # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤
            supported_extensions = ['*.docx', '*.xlsx', '*.DOCX', '*.XLSX']
            files_found = []
            
            for extension in supported_extensions:
                pattern = os.path.join(folder_path, extension)
                files_found.extend(glob.glob(pattern))
            
            if not files_found:
                return False, f"–í –ø–∞–ø–∫–µ {folder_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤ (.docx, .xlsx, .DOCX, .XLSX)"
            
            st.info(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(files_found)}")
            
            for file_path in files_found:
                file_extension = os.path.splitext(file_path)[1].lower()
                file_name = os.path.basename(file_path)
                
                try:
                    if file_extension == '.docx':
                        text = self._extract_text_from_docx(file_path)
                    elif file_extension == '.xlsx':
                        text = self._extract_text_from_xlsx(file_path)
                    else:
                        continue
                    
                    if text.strip():
                        doc = Document(
                            page_content=text, 
                            metadata={"source": file_name, "file_path": file_path}
                        )
                        all_documents.append(doc)
                        st.success(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω —Ñ–∞–π–ª: {file_name}")
                    else:
                        st.warning(f"–§–∞–π–ª {file_name} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞")
                        
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {file_name}: {str(e)}")
                    continue
            
            if not all_documents:
                return False, "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –Ω–∏ –∏–∑ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"
            
            # –†–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                separators=["\n\n", "\n", ". ", " ", ""]
            )
            
            split_documents = text_splitter.split_documents(all_documents)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–æ–≤
            bm25 = BM25Retriever.from_documents(split_documents)
            bm25.k = 4
            faiss = FAISS.from_documents(split_documents, self.embeddings)
            retriever = faiss.as_retriever(
                search_type="similarity",
                k=4,
                score_threshold=None,
            )

            self.vectorstore = EnsembleRetriever(
                retrievers=[bm25, retriever],
                weights=[0.5, 0.5],
            )
            
            self._create_qa_chain()
            
            return True, f"–î–æ–∫—É–º–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã. –°–æ–∑–¥–∞–Ω–æ {len(split_documents)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏–∑ {len(all_documents)} —Ñ–∞–π–ª–æ–≤."
            
        except Exception as e:
            return False, f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {str(e)}"
    
    def _extract_text_from_docx(self, file_path):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ DOCX —Ñ–∞–π–ª–∞"""
        try:
            # DOCX —Ñ–∞–π–ª - —ç—Ç–æ ZIP –∞—Ä—Ö–∏–≤ —Å XML —Ñ–∞–π–ª–∞–º–∏
            with zipfile.ZipFile(file_path, 'r') as zip_file:
                # –ß–∏—Ç–∞–µ–º document.xml, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç
                if 'word/document.xml' in zip_file.namelist():
                    xml_content = zip_file.read('word/document.xml')
                    root = ET.fromstring(xml_content)
                    
                    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
                    text_parts = []
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
                    for paragraph in root.findall('.//{http://schemas.openxmlformats.org/wordprocessingml/2006/main}p'):
                        para_text = []
                        for text_elem in paragraph.findall('.//{http://schemas.openxmlformats.org/wordprocessingml/2006/main}t'):
                            if text_elem.text:
                                para_text.append(text_elem.text)
                        if para_text:
                            text_parts.append(''.join(para_text))
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Ç–∞–±–ª–∏—Ü
                    for table in root.findall('.//{http://schemas.openxmlformats.org/wordprocessingml/2006/main}tbl'):
                        for row in table.findall('.//{http://schemas.openxmlformats.org/wordprocessingml/2006/main}tr'):
                            row_text = []
                            for cell in row.findall('.//{http://schemas.openxmlformats.org/wordprocessingml/2006/main}tc'):
                                cell_text = []
                                for text_elem in cell.findall('.//{http://schemas.openxmlformats.org/wordprocessingml/2006/main}t'):
                                    if text_elem.text:
                                        cell_text.append(text_elem.text)
                                if cell_text:
                                    row_text.append(''.join(cell_text))
                            if row_text:
                                text_parts.append(" | ".join(row_text))
                    
                    return "\n".join(text_parts)
                else:
                    st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ document.xml –≤ —Ñ–∞–π–ª–µ {file_path}")
                    return ""
                    
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ DOCX —Ñ–∞–π–ª–∞ {file_path}: {str(e)}")
            return ""
    
    def _extract_text_from_xlsx(self, file_path):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ XLSX —Ñ–∞–π–ª–∞"""
        try:
            # –ß–∏—Ç–∞–µ–º –≤—Å–µ –ª–∏—Å—Ç—ã Excel —Ñ–∞–π–ª–∞
            excel_file = pd.ExcelFile(file_path)
            text_parts = []
            
            for sheet_name in excel_file.sheet_names:
                df = pd.read_excel(file_path, sheet_name=sheet_name)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –ª–∏—Å—Ç–∞
                text_parts.append(f"–õ–∏—Å—Ç: {sheet_name}")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
                if not df.empty:
                    headers = df.columns.tolist()
                    text_parts.append("–ó–∞–≥–æ–ª–æ–≤–∫–∏: " + " | ".join([str(h) for h in headers]))
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ (–ø–µ—Ä–≤—ã–µ 100 —Å—Ç—Ä–æ–∫ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞)
                    for index, row in df.head(100).iterrows():
                        row_data = [str(cell) for cell in row.values if pd.notna(cell) and str(cell).strip()]
                        if row_data:
                            text_parts.append(" | ".join(row_data))
                
                text_parts.append("")  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –º–µ–∂–¥—É –ª–∏—Å—Ç–∞–º–∏
            
            return "\n".join(text_parts)
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ XLSX —Ñ–∞–π–ª–∞ {file_path}: {str(e)}")
            return ""
    
    def _create_qa_chain(self):
        """–°–æ–∑–¥–∞–µ—Ç —Ü–µ–ø–æ—á–∫—É –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç"""
        template = """–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å–ª–µ–¥—É—é—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å. 
        –ï—Å–ª–∏ —Ç—ã –Ω–µ –∑–Ω–∞–µ—à—å –æ—Ç–≤–µ—Ç–∞, —Å–∫–∞–∂–∏, —á—Ç–æ –Ω–µ –∑–Ω–∞–µ—à—å, –Ω–µ –ø—ã—Ç–∞–π—Å—è –ø—Ä–∏–¥—É–º–∞—Ç—å –æ—Ç–≤–µ—Ç.
        –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ç–æ–º –∂–µ —è–∑—ã–∫–µ, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –∑–∞–¥–∞–Ω –≤–æ–ø—Ä–æ—Å.

        –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}

        –í–æ–ø—Ä–æ—Å: {question}

        –û—Ç–≤–µ—Ç:"""
        
        prompt = PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )
        
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore,
            chain_type_kwargs={"prompt": prompt},
            return_source_documents=True
        )
    
    def ask_question(self, question):
        """–ó–∞–¥–∞–µ—Ç –≤–æ–ø—Ä–æ—Å –∞–≥–µ–Ω—Ç—É"""
        if not self.qa_chain:
            return "–û—à–∏–±–∫–∞: –î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –ø–∞–ø–∫–∏."
        
        try:
            result = self.qa_chain.invoke({"query": question})
            answer = result["result"]
            sources = result["source_documents"]
            return answer, sources
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–æ–ø—Ä–æ—Å–∞: {str(e)}", []

def main():
    load_dotenv()
    openai_api_key = os.getenv("OPENAI_API_KEY")

    if 'agent' not in st.session_state:
        try:
            st.session_state.agent = DocumentAgent(openai_api_key)
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–∞: {str(e)}")
            st.stop()
    
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    
    if 'documents_loaded' not in st.session_state:
        st.session_state.documents_loaded = False

    st.set_page_config(
        page_title="Document Folder QA Agent",
        layout="wide"
    )

    st.markdown("<h2 style='text-align: center; color: white;'>Document Folder QA Agent</h2>", unsafe_allow_html=True)
    st.markdown("<h6 style='text-align: center; color: white;'>–ê–≥–µ–Ω—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –ø–∞–ø–∫–∏</h6>", unsafe_allow_html=True)

    with st.sidebar:
        st.markdown("<h2 style='text-align: center; color: white;'>–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤</h2>", unsafe_allow_html=True)

        # –í—ã–±–æ—Ä –ø–∞–ø–∫–∏
        folder_option = st.radio(
            "–í—ã–±–µ—Ä–∏—Ç–µ —Å–ø–æ—Å–æ–± –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:",
            ("–£–∫–∞–∑–∞—Ç—å –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ", "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–∞–ø–∫—É –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
        )
        
        folder_path = None
        
        if folder_option == "–£–∫–∞–∑–∞—Ç—å –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ":
            folder_path = st.text_input(
                "–í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏:",
                placeholder="/path/to/documents",
                help="–£–∫–∞–∂–∏—Ç–µ –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ, —Å–æ–¥–µ—Ä–∂–∞—â–µ–π .docx –∏ .xlsx —Ñ–∞–π–ª—ã"
            )
        else:
            # –ü–∞–ø–∫–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é - —Å–æ–∑–¥–∞–µ–º –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            default_folder = "documents"
            if not os.path.exists(default_folder):
                os.makedirs(default_folder)
                st.info(f"–°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {default_folder}")
                st.info("–ü–æ–º–µ—Å—Ç–∏—Ç–µ –≤–∞—à–∏ .docx –∏ .xlsx —Ñ–∞–π–ª—ã –≤ —ç—Ç—É –ø–∞–ø–∫—É")
            folder_path = default_folder
            st.success(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–∞–ø–∫–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {folder_path}")
        
        if folder_path and st.button("–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã"):
            if os.path.exists(folder_path):
                with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤..."):
                    if hasattr(st.session_state, 'agent'):
                        success, message = st.session_state.agent.load_documents_from_folder(folder_path)
                        if success:
                            st.success(message)
                            st.session_state.documents_loaded = True
                        else:
                            st.error(message)
                            st.session_state.documents_loaded = False
                    else:
                        st.error("–ê–≥–µ–Ω—Ç –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É.")
            else:
                st.error(f"–ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {folder_path}")

    st.markdown("<h2 style='text-align: center; color: white;'>–ß–∞—Ç —Å –∞–≥–µ–Ω—Ç–æ–º</h2>", unsafe_allow_html=True)

    if not hasattr(st.session_state, 'documents_loaded') or not st.session_state.documents_loaded:
        st.info("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É–∫–∞–∂–∏—Ç–µ –ø–∞–ø–∫—É —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–π—Ç–µ –∏—Ö –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã.")
        return

    chat_container = st.container()
    
    with chat_container:
        for i, (question, answer) in enumerate(st.session_state.chat_history):
            with st.chat_message("user"):
                st.write(question)
            with st.chat_message("assistant"):
                st.write(answer)

        question = st.chat_input("–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º...")
        
        if question:
            with st.chat_message("user"):
                st.write(question)

            with st.chat_message("assistant"):
                with st.spinner("–î—É–º–∞—é..."):
                    answer, sources = st.session_state.agent.ask_question(question)
                    answer = answer.split("</think>")[-1].strip()
                    st.write(answer)
                    st.session_state.chat_history.append((question, answer))

                    if sources and len(sources) > 0:
                        with st.expander("üìñ –ò—Å—Ç–æ—á–Ω–∏–∫–∏"):
                            for i, source in enumerate(sources):
                                source_info = f"–§–∞–π–ª: {source.metadata.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}"
                                st.text_area(
                                    f"–ò—Å—Ç–æ—á–Ω–∏–∫ {i+1} ({source_info}):",
                                    source.page_content,
                                    height=100
                                )

if __name__ == "__main__":
    main() 